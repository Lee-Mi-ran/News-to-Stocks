import gradio as gr
import chromadb
import openai
import pandas as pd
import tiktoken
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import schedule
from datetime import datetime, timedelta

# ğŸ”¹ OpenAI API ì„¤ì •
openai_client = openai.OpenAI(api_key="OPEN_AI_KEY")  # OpenAI API í‚¤ ì…ë ¥

# ğŸ”¹ ChromaDB ì„¤ì •
# ğŸ”¹ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = chromadb.PersistentClient(path="./chroma_db_sp500")
collection = client.get_or_create_collection("sp500")

# ğŸ”¹ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸° ë° ì €ì¥ í•¨ìˆ˜
def store_sp500_in_chromadb():
    df = pd.read_csv("sp500_companies.csv")  # âœ… S&P 500 ê¸°ì—… ì •ë³´ CSV íŒŒì¼ ì½ê¸°
    existing_count = collection.count()

    # âœ… ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
    if existing_count > 0:
        print(f"âœ… ê¸°ì¡´ ê¸°ì—… ë°ì´í„° ({existing_count}ê°œ)ê°€ ì¡´ì¬í•˜ì—¬ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    print("ğŸŸ¡ S&P 500 ê¸°ì—… ì •ë³´ë¥¼ ChromaDBì— ì €ì¥ ì¤‘...")

    # âœ… OpenAI ì„ë² ë”© ìƒì„± í•¨ìˆ˜
    def get_embedding(text):
        enc = tiktoken.encoding_for_model("text-embedding-ada-002")
        tokens = enc.encode(text)
        if len(tokens) > 7000:  # ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
            tokens = tokens[:7000]
            text = enc.decode(tokens)

        response = openai_client.embeddings.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response.data[0].embedding

    # âœ… ë°ì´í„° ì €ì¥
    for idx, row in df.iterrows():
        company_name = row["company_name"]
        overview = row["overview"]

        # âœ… ë¹ˆ ë°ì´í„°ëŠ” ì œì™¸
        if not overview.strip():
            continue

        # âœ… ì„ë² ë”© ìƒì„±
        embedding = get_embedding(overview)

        # âœ… ChromaDBì— ì €ì¥
        collection.add(
            ids=[str(idx)],
            embeddings=[embedding],
            metadatas=[{"company_name": company_name, "overview": overview}]
        )

    print("âœ… S&P 500 ê¸°ì—… ì •ë³´ ì €ì¥ ì™„ë£Œ!")


news_df = None

# ğŸ”¹ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
# ë‰´ìŠ¤ ê¸°ì‚¬ í¬í•¨ (Title, Content ì»¬ëŸ¼ í¬í•¨)
def crawl_yahoo_news():
    print("ë‰´ìŠ¤ í¬ë¡¤ë§ì¤‘...")
    # âœ… Yahoo Finance Stock Market News í˜ì´ì§€ URL
    URL = "https://finance.yahoo.com/topic/stock-market-news/"

    # âœ… Selenium WebDriver ì„¤ì •
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€
    options.add_argument("--no-sandbox")  # ì¶©ëŒ ë°©ì§€
    options.add_argument("--disable-dev-shm-usage")  # ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°
    options.add_argument("--disable-gpu")  # GPU ê°€ì† ë¹„í™œì„±í™”
    options.add_argument("user-agent=Mozilla/5.0")

    # âœ… WebDriver ì‹¤í–‰
    driver = webdriver.Chrome(options=options)
    driver.get(URL)

    # âœ… í˜ì´ì§€ ì™„ì „ ë¡œë“œ ëŒ€ê¸°
    time.sleep(10)

    # âœ… ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ URL ì €ì¥
    seen_links = set()
    news_data = []

    # âœ… ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 5ê°œ)
    try:
        articles = WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located(
                (By.XPATH, '//div[contains(@class, "topic-stories")]//a[contains(@href, "/news/")]')
            )
        )

        for article in articles:
            link = article.get_attribute("href").strip()

            # âœ… ì¤‘ë³µëœ ê¸°ì‚¬ URL ìŠ¤í‚µ
            if link in seen_links:
                continue
            seen_links.add(link)

            # ìƒˆ ì°½ì—ì„œ ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
            driver.execute_script("window.open(arguments[0]);", link)
            driver.switch_to.window(driver.window_handles[1])
            time.sleep(5)

            # âœ… ê¸°ì‚¬ ì œëª© (cover-title)
            title = "Unknown"
            try:
                title_element = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "cover-title"))
                )
                title = title_element.text.strip()
            except:
                pass

            # âœ… ì–¸ë¡ ì‚¬ (ì¶œì²˜)
            publisher = "Unknown"
            try:
                publisher_element = driver.find_elements(By.XPATH, '//a[@class="subtle-link fin-size-small yf-1xqzjha"]')
                if publisher_element:
                    publisher = publisher_element[0].get_attribute("title").strip()
            except:
                pass

            # âœ… ë‚ ì§œ í¬ë¡¤ë§ ë° ë³€í™˜
            us_date, ko_date = "Unknown", "Unknown"
            try:
                time_element = driver.find_elements(By.CLASS_NAME, "byline-attr-meta-time")
                if time_element:
                    iso_timestamp = time_element[0].get_attribute("datetime")
                    dt_obj = datetime.strptime(iso_timestamp, "%Y-%m-%dT%H:%M:%S.%fZ")
                    us_date = dt_obj.strftime('%Y-%m-%d %H:%M')  # âœ… UTC ì‹œê°„
                    ko_date = (dt_obj + timedelta(hours=9)).strftime('%Y-%m-%d %H:%M')  # âœ… í•œêµ­ ì‹œê°„
            except:
                pass

            # âœ… ê¸°ì‚¬ ë³¸ë¬¸ (atoms-wrapper ë‚´ë¶€ p íƒœê·¸)
            content = "Unknown"
            try:
                paragraphs = driver.find_elements(By.XPATH, '//div[@class="atoms-wrapper"]//p')
                if paragraphs:
                    content = "\n".join([p.text.strip() for p in paragraphs if p.text.strip()])
            except:
                pass

            # âœ… ë°ì´í„° ì €ì¥
            news_data.append({
                "Title": title,
                "URL": link,
                "Publisher": publisher,
                "US_Date": us_date,
                "KO_Date": ko_date,
                "Content": content
            })

            # ìƒˆ ì°½ ë‹«ê³  ì›ë˜ ì°½ìœ¼ë¡œ ë³µê·€
            driver.close()
            driver.switch_to.window(driver.window_handles[0])

            # âœ… 5ê°œ ê¸°ì‚¬ê¹Œì§€ë§Œ ì €ì¥
            if len(news_data) >= 5:
                break

    except Exception as e:
        print(f"âŒ XPath ë¬¸ì œ ë°œìƒ: {e}")

    # âœ… DataFrame ìƒì„± í›„ ì €ì¥
    df = pd.DataFrame(news_data)
    if df.empty:
        print("âŒ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
    else:
        df.to_csv("yahoo_finance_news.csv", index=False, encoding="utf-8")
        print("âœ… Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ! â€˜yahoo_finance_news.csvâ€™ íŒŒì¼ ì €ì¥ë¨.")

    # âœ… WebDriver ì¢…ë£Œ
    driver.quit()
    store_news_in_chromadb()
    
# ğŸ”¹ í† í° ìˆ˜ ì œí•œ í•¨ìˆ˜
def truncate_text_by_tokens(text, max_tokens=7000):
    enc = tiktoken.encoding_for_model("text-embedding-ada-002")
    tokens = enc.encode(text)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
        text = enc.decode(tokens)
    return text

# ğŸ”¹ OpenAI ì„ë² ë”© ìƒì„±
def get_embedding(text):
    text = truncate_text_by_tokens(text, max_tokens=7000)
    response = openai_client.embeddings.create(
        input=text,
        model="text-embedding-ada-002"
    )
    return response.data[0].embedding

# ğŸ”¹ ê¸°ì‚¬ ë²ˆì—­ ë° ìš”ì•½
def translate_and_summarize(text):
    prompt = f"""
    ì•„ë˜ ì˜ì–´ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ í›„ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ê³µë°±í¬í•¨ ê¸€ì ìˆ˜ 400ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë‹¤.ë¼ê³  ëë‚˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    {text}
    """

    response = openai_client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500
    )

    return response.choices[0].message.content

# ğŸ”¹ ìœ ì‚¬í•œ ê¸°ì—… ì°¾ê¸°
def find_similar_companies(content, top_n=5):
    query_embedding = get_embedding(content)

    search_result = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_n
    )

    if "metadatas" not in search_result or not search_result["metadatas"]:
        return ["âš ï¸ ìœ ì‚¬í•œ ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ì—†ìŒ)"]

    if search_result["metadatas"][0] is None:
        return ["âš ï¸ ìœ ì‚¬í•œ ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (NoneType ì˜¤ë¥˜)"]

    best_matches = []
    for i, res in enumerate(search_result["metadatas"][0]):
        company_name = res.get("company_name", "ì •ë³´ ì—†ìŒ")
        similarity_score = search_result["distances"][0][i] if "distances" in search_result else 1.0
        similarity_percentage = round((1 - similarity_score) * 100, 2)  # 100ì  ë§Œì ìœ¼ë¡œ ë³€í™˜
        
        if isinstance(company_name, list):
            for name in company_name:
                best_matches.append(f"{name} ({similarity_percentage}%)")
        elif isinstance(company_name, str):
            best_matches.append(f"{company_name} ({similarity_percentage}%)")

    return list(set(best_matches)) if best_matches else ["âš ï¸ ìœ ì‚¬í•œ ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

# âœ… **1ï¸âƒ£ S&P ë°ì´í„° ì €ì¥ ì—¬ë¶€ í™•ì¸ í›„ ì¶”ê°€**
def store_news_in_chromadb():
    global news_df
    global collection
    news_df = pd.read_csv('yahoo_finance_news.csv')
    existing_count = collection.count()
    if existing_count > 0:
        print(f"âœ… ê¸°ì¡´ ë°ì´í„° ({existing_count}ê°œ)ê°€ ì¡´ì¬í•˜ì—¬ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    print("ğŸŸ¡ ChromaDB ë°ì´í„° ì €ì¥ ì¤‘...")
    
    for idx, row in news_df.iterrows():
        title = row["Title"]
        content = row["Content"]

        if not content.strip():
            continue

        embedding = get_embedding(content)
        company_names = find_similar_companies(content)

        collection.add(
            ids=[str(idx)],
            embeddings=[embedding],
            metadatas=[{"title": title, "content": content, "company_name": company_names}]
        )
    
    print("âœ… ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")

# âœ… **2ï¸âƒ£ ì²˜ìŒ ì‹¤í–‰ ì‹œ ê¸°ì‚¬ ì •ë³´ ìë™ ì¶œë ¥**
def initialize_chat():
    chat_history = []

    for idx in range(len(news_df)):
        title = news_df['Title'][idx]
        link = news_df['URL'][idx]
        content = news_df['Content'][idx]

        if not content.strip():
            chat_history.append(("ğŸ“¢ ì‹œìŠ¤í…œ", f"ğŸ”¹ {title}: âš ï¸ ê¸°ì‚¬ ë‚´ìš© ì—†ìŒ"))
            continue

        similar_companies = find_similar_companies(content, top_n=5)
        translated_summary = translate_and_summarize(content)

        response_message = f"""ğŸ”¹ **{title}**
ğŸ“Œ **ê´€ë ¨ ê¸°ì—…**: {', '.join(similar_companies)}

ğŸ“œ **í•œê¸€ ìš”ì•½**:
{translated_summary}

**ë§í¬**
{link}
"""

        chat_history.append(("ğŸ¤– AI", response_message))

    chat_history.append(("ğŸ“¢ ì‹œìŠ¤í…œ", "ğŸ’¬ ê¸°ì‚¬ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì§ˆë¬¸í•˜ì„¸ìš”!"))

    return chat_history

# âœ… **3ï¸âƒ£ ìœ ì €ê°€ ì§ˆë¬¸í•˜ë©´ AIê°€ ë‹µë³€**
def chatbot(user_input, chat_history):
    if user_input:
        chat_history.append((user_input, "ğŸ‘¤ ì‚¬ìš©ì"))

        similar_companies = find_similar_companies(user_input, top_n=5)
        similar_companies_text = f"ê´€ë ¨ ê¸°ì—…: {', '.join(similar_companies)}" if similar_companies else "ê´€ë ¨ ê¸°ì—… ì •ë³´ ì—†ìŒ"
        
        prompt = f"""
        ì‚¬ìš©ì ì§ˆë¬¸: {user_input}
        
        ê´€ë ¨ ê¸°ì—… ì •ë³´: {similar_companies_text}
        
        ì•„ë˜ ê¸°ì‚¬ ë° ê´€ë ¨ ê¸°ì—… ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

        {news_df[['Title', 'Content']].to_string(index=False)}
        
        ë‚´ìš©ì„ ê³µë°± í¬í•¨ 400ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ê³ , ë‹¤.ë¡œ ëë‚´ì£¼ì„¸ìš”.
        íŠ¹ìˆ˜ê¸°í˜¸ëŠ” ë¹¼ì£¼ì„¸ìš”. ì´ëª¨ì§€ë„ ë¹¼ì£¼ì„¸ìš”.
        """

        response = openai_client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500
        )

        chat_history.append(("ğŸ¤– AI", response.choices[0].message.content))

    return chat_history

init_history = None

def before_func():
    global init_history
    store_sp500_in_chromadb()
    crawl_yahoo_news()
    init_history = initialize_chat()
    
# âœ… Gradio ì‹¤í–‰ (Chatbot ìŠ¤íƒ€ì¼ + ì‚¬ìš©ì ì…ë ¥ ì§€ì›)
def main_func():
    with gr.Blocks() as demo:
        gr.Markdown("# ğŸ“Š AI ë‰´ìŠ¤ ë¶„ì„ ë° ê¸°ì—… ì¶”ì²œ ì±—ë´‡")
        gr.Markdown("ğŸ“¢ ë‰´ìŠ¤ ì œëª©, ê´€ë ¨ ê¸°ì—…, ìš”ì•½ì„ ë¨¼ì € ì œê³µí•œ í›„ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        chatbot_ui = gr.Chatbot(value=init_history, elem_id="chatbot_ui")
        user_input = gr.Textbox(label="ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ê¸°ì‚¬ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...")
        submit_btn = gr.Button("ì§ˆë¬¸í•˜ê¸°")
        
        submit_btn.click(chatbot, inputs=[user_input, chatbot_ui], outputs=chatbot_ui)
    
    demo.launch()

schedule.every().hour.at(":55").do(before_func)
schedule.every().hour.at(":00").do(main_func)

# ì£¼ê¸°ì ìœ¼ë¡œ ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ë¬´í•œ ë£¨í”„

while True:
    schedule.run_pending()  # ì˜ˆì•½ëœ ì‘ì—…ì„ ì‹¤í–‰
    time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°


# before_func()
# main_func()
